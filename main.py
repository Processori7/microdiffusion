"""
–ú–∏–∫—Ä–æ Stable Diffusion - –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å —Ç–µ–æ—Ä–∏–µ–π –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
"""

# –¢–µ–æ—Ä–∏—è: 
# –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞:
# 1. –ü—Ä—è–º–æ–π –ø—Ä–æ—Ü–µ—Å—Å (–¥–∏—Ñ—Ñ—É–∑–∏—è) - –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
# 2. –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å - –æ–±—É—á–∞–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç—å –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —à—É–º–∞
# –ú—ã —Ä–µ–∞–ª–∏–∑—É–µ–º —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞ —Å–ª–∞–±–æ–º –∂–µ–ª–µ–∑–µ
import torch
import torch.nn as nn
import torch.optim as optim
import os
import numpy as np
import customtkinter as ctk
import matplotlib.pyplot as plt
import requests
import time
import torchvision.transforms as transforms

from duckduckgo_search import DDGS
from sklearn.feature_extraction.text import TfidfVectorizer
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from PIL import Image, ImageTk
from tqdm import tqdm
from datetime import datetime
from io import BytesIO

def train_model_with_mixed_precision(model, dataloader, diffusion, criterion, optimizer, device, target_loss=0.02, max_epochs=150):
    """
    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º mixed precision (FP16)
    
    :param model: –Ω–µ–π—Ä–æ—Å–µ—Ç—å (TinyUNet –∏–ª–∏ —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    :param dataloader: –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞
    :param diffusion: –ø—Ä–æ—Ü–µ—Å—Å –¥–∏—Ñ—Ñ—É–∑–∏–∏
    :param criterion: —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (MSE)
    :param optimizer: –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    :param device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cuda/cpu)
    :param target_loss: —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ loss (~98% —Ç–æ—á–Ω–æ—Å—Ç–∏)
    :param max_epochs: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —ç–ø–æ—Ö
    """
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å Mixed Precision (FP16)")
    
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff'}

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏ –ø–∞–ø–æ–∫ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    files = os.listdir(output_dir)

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    image_count = sum(1 for f in files if os.path.isfile(os.path.join(output_dir, f)) and os.path.splitext(f)[1].lower() in image_extensions)

    batches_per_epoch = image_count // 4
    total_batches = batches_per_epoch * max_epochs
    total_time_sec = total_batches * 0.4
    print(f"üìä –ü—Ä–æ–≥–Ω–æ–∑ –≤—Ä–µ–º–µ–Ω–∏:")
    print(f"  ‚Ä¢ {image_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"  ‚Ä¢ {max_epochs} —ç–ø–æ—Ö")
    print(f"  ‚Ä¢ {4} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ –ø—Ä–æ—Ö–æ–¥")
    print(f"  ‚Ä¢ ~0.4 —Å–µ–∫—É–Ω–¥ –Ω–∞ –±–∞—Ç—á")
    print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time_sec:.2f} —Å–µ–∫—É–Ω–¥ (~{total_time_sec//60} –º–∏–Ω)")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GradScaler –¥–ª—è FP16
    scaler = torch.amp.GradScaler('cuda') if torch.cuda.is_available() else torch.amp.GradScaler('cpu')
    best_loss = float('inf')

    start_time = time.time()

    for epoch in range(max_epochs):
        total_loss = 0
        model.train()  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
        
        # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–µ
        print(f"üîÅ –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –≠–ø–æ—Ö–∞ {epoch + 1} / {max_epochs}")
        
        for images, _ in dataloader:
            images = images.to(device)
            t = torch.randint(0, diffusion.T, (images.shape[0],)).to(device)

            with autocast():  # –í–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç—å—é
                x_t, noise = diffusion.forward(images, t)
                predicted_noise = model(x_t, t)
                loss = criterion(predicted_noise, noise)

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()
        end_time = time.time()
        total_duration = end_time - start_time
        print(f"‚è±Ô∏è –û–±—É—á–µ–Ω–∏–µ –∑–∞–Ω—è–ª–æ {total_duration:.2f} —Å–µ–∫—É–Ω–¥")

        avg_loss = total_loss / len(dataloader)
        print(f"‚úÖ –≠–ø–æ—Ö–∞ {epoch + 1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | –°—Ä–µ–¥–Ω–∏–π Loss: {avg_loss:.4f}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), "tiny_diffusion.pth")
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å Loss: {best_loss:.4f}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        if avg_loss < target_loss:
            print(f"üéØ –¶–µ–ª–µ–≤–æ–π loss ({target_loss}) –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}")
            break

    print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

def download_images_from_ddgs_results(results, output_dir="dataset/street_art", image_size=(128, 128), max_count=2000):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏ —Ä–µ—Å–∞–π–∑–∏—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ DDGS.images()
    –ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –ø–æ–ø—ã—Ç–∫–∏, –ø–æ–∫–∞ –Ω–µ –±—É–¥–µ—Ç —Å–∫–∞—á–∞–Ω–æ max_count –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
    """
    os.makedirs(output_dir, exist_ok=True)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff'}

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏ –ø–∞–ø–æ–∫ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    files = os.listdir(output_dir)

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    image_count = sum(1 for f in files if os.path.isfile(os.path.join(output_dir, f)) and os.path.splitext(f)[1].lower() in image_extensions)
    
    downloaded = image_count
    attempts = 0
    max_attempts_per_image = 1  # –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –Ω–∞ –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ

    print(f"üîç –ü—ã—Ç–∞—é—Å—å —Å–∫–∞—á–∞—Ç—å {max_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")

    while downloaded < max_count:
        for idx, item in enumerate(results):
            if downloaded >= max_count:
                break

            img_url = item["image"]
            success = False
            attempt = 0

            while not success and attempt < max_attempts_per_image:
                try:
                    response = requests.get(img_url, headers=headers, timeout=5)
                    response.raise_for_status()

                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    img = Image.open(BytesIO(response.content)).convert("RGB")
                    img = img.resize(image_size)

                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                    filename = f"{downloaded:04d}_{item['title'][:50].replace(' ', '_')}.jpg"
                    safe_filename = "".join(c for c in filename if c.isalnum() or c in ("_", ".", "-"))
                    save_path = os.path.join(output_dir, safe_filename)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                    img.save(save_path)
                    downloaded += 1
                    print(f"[OK] –°–∫–∞—á–∞–Ω–æ: {safe_filename}")
                    success = True

                except Exception as e:
                    attempt += 1
                    print(f"[–û—à–∏–±–∫–∞] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å: {img_url} | –ü—Ä–∏—á–∏–Ω–∞: {e} | –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{max_attempts_per_image}")

            attempts += 1
            time.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ {downloaded} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ '{output_dir}'")

# 1. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏
class TinyUNet(nn.Module):
    def __init__(self):
        super().__init__()

        # Encoder
        self.encoder1 = self.conv_block(3, 16)
        self.encoder2 = self.conv_block(16, 32)
        self.encoder3 = self.conv_block(32, 64)

        # Middle
        self.middle = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU()
        )

        # Decoder
        self.decoder3 = self.upconv_block(64, 32)
        self.decoder2 = self.upconv_block(32 + 32, 16)  # skip connection
        self.final = nn.Sequential(
            nn.ConvTranspose2d(16 + 16, 3, kernel_size=2, stride=2),
            nn.Tanh()
        )  # –ß—Ç–æ–±—ã –≤—ã—Ö–æ–¥ –±—ã–ª [-1, 1]

    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )

    def upconv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),
            nn.BatchNorm2d(out_channels),
            nn.ReLU()
        )

    def forward(self, x, t):
        # print(f"Input shape: {x.shape}")  # [batch, 3, 128, 128]

        # Encoder
        e1 = self.encoder1(x)
        # print(f"e1 shape: {e1.shape}")  # [batch, 16, 64, 64]

        e2 = self.encoder2(e1)
        # print(f"e2 shape: {e2.shape}")  # [batch, 32, 32, 32]

        e3 = self.encoder3(e2)
        # print(f"e3 shape: {e3.shape}")  # [batch, 64, 16, 16]

        # Middle
        m = self.middle(e3)
        # print(f"Middle shape: {m.shape}")  # [batch, 64, 16, 16]

        # Decoder 3
        d3 = self.decoder3(m)
        # print(f"d3 shape: {d3.shape}")  # [batch, 32, 32, 32]

        # Skip connection
        # print(f"e2 shape: {e2.shape}")  # [batch, 32, 32, 32]
        d3 = torch.cat([d3, e2], dim=1)
        # print(f"After cat(d3, e2): {d3.shape}")  # [batch, 64, 32, 32]

        # Decoder 2
        d2 = self.decoder2(d3)
        # print(f"d2 shape: {d2.shape}")  # [batch, 16, 64, 64]

        # Skip connection
        # print(f"e1 shape: {e1.shape}")  # [batch, 16, 64, 64]
        d2 = torch.cat([d2, e1], dim=1)
        # print(f"After cat(d2, e1): {d2.shape}")  # [batch, 32, 64, 64]

        # Final layer to get back to input size
        output = self.final(d2)
        # print(f"Final output shape: {output.shape}")  # [batch, 3, 128, 128]

        return output

# 2. –¢–µ–∫—Å—Ç–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä
class TextEncoder:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
    
    –ò—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
    """
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=128)
        
    def fit(self, texts):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏—è—Ö"""
        self.vectorizer.fit(texts)
        
    def encode(self, text):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä"""
        return torch.tensor(self.vectorizer.transform([text]).toarray()).float()

# 3. –ü—Ä–æ—Ü–µ—Å—Å –¥–∏—Ñ—Ñ—É–∑–∏–∏
class DiffusionProcess:
    """
    –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
    
    –¢–µ–æ—Ä–∏—è:
    –ü—Ä—è–º–æ–π –ø—Ä–æ—Ü–µ—Å—Å –¥–∏—Ñ—Ñ—É–∑–∏–∏: q(x_t | x_{t-1}) = N(x_t; sqrt(Œ±_t)x_{t-1}, (1-Œ±_t)I)
    –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å: p_Œ∏(x_{t-1} | x_t) = N(x_{t-1}; Œº_Œ∏(x_t, t), œÉ^2I)
    """
    def __init__(self, T=150):
        self.T = T
        self.betas = torch.linspace(0.0001, 0.02, T)  # –®—É–º –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
        self.alphas = 1 - self.betas
        self.alpha_hats = torch.cumprod(self.alphas, dim=0)  # –ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –¥–ª—è –æ–±—â–µ–≥–æ —à—É–º–∞

    def forward(self, x, t):
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ü–µ—Å—Å: –¥–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
        x: –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        t: –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥
        """
        noise = torch.randn_like(x)
        sqrt_alpha_hat = torch.sqrt(self.alpha_hats[t]).view(-1, 1, 1, 1)  # –†–∞—Å—à–∏—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hats[t]).view(-1, 1, 1, 1)
        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * noise, noise

    def sample(self, model, n, size):
        """
        –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑ —à—É–º–∞
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        n: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        size: —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        device = next(model.parameters()).device
        x = torch.randn(n, 3, *size).to(device)

        for t in reversed(range(self.T)):
            noise_pred = model(x, t)
            alpha = self.alphas[t]
            alpha_hat = self.alpha_hats[t]

            x = 1 / torch.sqrt(alpha) * (x - (1 - alpha) / torch.sqrt(1 - alpha_hat) * noise_pred)
            if t > 0:
                x += torch.sqrt(self.betas[t]) * torch.randn_like(x)

        return x.clamp(-1, 1)

# 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
class ArtDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.files = [f for f in os.listdir(root_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]

    def __getitem__(self, idx):
        img_path = os.path.join(self.root_dir, self.files[idx])
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)  # ‚Üê –ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Compose(), –∞ –Ω–µ –º–æ–¥—É–ª—å
        
        return image, "street art"
    
    def __len__(self):
        return len(self.files)

# 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def train_model(target_loss=0.1, max_epochs=100):
    """
    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å, –ø–æ–∫–∞ loss –Ω–µ —Å—Ç–∞–Ω–µ—Ç –º–µ–Ω—å—à–µ target_loss –∏–ª–∏ –Ω–µ –∑–∞–∫–æ–Ω—á–∞—Ç—Å—è —ç–ø–æ—Ö–∏.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - target_loss: —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, 0.02 ‚âà 98% "—Ç–æ—á–Ω–æ—Å—Ç–∏")
    - max_epochs: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
    """
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")

    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    dataset = ArtDataset("dataset/street_art", transform=transform)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    model = TinyUNet()
    diffusion = DiffusionProcess(T=100)
    text_encoder = TextEncoder()
    text_encoder.fit(["street art", "graffiti", "urban art"])

    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.MSELoss()

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
    if os.path.exists("tiny_diffusion.pth"):
        model.load_state_dict(torch.load("tiny_diffusion.pth"))
        print("‚úÖ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏")

    train_model_with_mixed_precision(model, dataloader, diffusion, criterion, optimizer, device, target_loss=target_loss, max_epochs=max_epochs)

    print("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

# 6. –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
class DiffusionGUI(ctk.CTk):
    """
    –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –í–≤–æ–¥ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    - –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    """
    def __init__(self):
        super().__init__()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        if not os.path.exists("tiny_diffusion.pth"):
            print("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
            train_model()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–∫–Ω–∞
        self.title("–ú–∏–∫—Ä–æ Stable Diffusion")
        self.geometry("800x600")
        ctk.set_appearance_mode("dark")
        ctk.set_default_color_theme("blue")
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
        self.create_widgets()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.init_model()
    
    def create_widgets(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        self.title_label = ctk.CTkLabel(
            self, 
            text="–ú–∏–∫—Ä–æ Stable Diffusion", 
            font=ctk.CTkFont(size=24, weight="bold")
        )
        self.title_label.pack(pady=20)
        
        # –ü–æ–ª–µ –≤–≤–æ–¥–∞
        self.prompt_entry = ctk.CTkEntry(
            self, 
            placeholder_text="–í–≤–µ–¥–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ...",
            width=400
        )
        self.prompt_entry.pack(pady=10)
        
        # –ö–Ω–æ–ø–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self.generate_button = ctk.CTkButton(
            self, 
            text="–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å",
            command=self.generate_image
        )
        self.generate_button.pack(pady=10)
        
        # –•–æ–ª—Å—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        self.canvas = ctk.CTkCanvas(
            self, 
            width=512, 
            height=512,
            bg="#2b2b2b"
        )
        self.canvas.pack(pady=20)
    
    def init_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        self.model = TinyUNet()
        self.model.load_state_dict(torch.load("tiny_diffusion.pth"))
        self.diffusion = DiffusionProcess()
        
        # –ü–µ—Ä–µ–≤–æ–¥ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
        self.model.eval()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
    
    def generate_image(self):
        prompt = self.prompt_entry.get()
        print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {prompt}")
        with torch.no_grad():
            generated = self.diffusion.sample(self.model, 1, (64, 64))
        image = transforms.ToPILImage()(generated[0].cpu() * 0.5 + 0.5)
        display_image = image.resize((512, 512))

        os.makedirs("generated", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = os.path.join("generated", f"{timestamp}_{prompt.replace(' ', '_')}.png")
        image.save(filename)

        self.tk_image = ImageTk.PhotoImage(display_image)
        self.canvas.create_image(0, 0, anchor="nw", image=self.tk_image)
        self.canvas.image = self.tk_image
        print("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ!")


if __name__ == "__main__":
    output_dir = "dataset/street_art"
    # –®–∞–≥ 1: –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ DuckDuckGo
    # results = DDGS().images(
    #     keywords="street art graffiti urban art",
    #     region="wt-wt",
    #     safesearch="on"
    # )

    # –®–∞–≥ 2: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    # download_images_from_ddgs_results(
    #     results,
    #     output_dir=output_dir,
    #     image_size=(128, 128),
    #     max_count=2000
    # )

    # –®–∞–≥ 3: –°–æ–∑–¥–∞—ë–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
    transform = transforms.Compose([
        transforms.Resize((128, 128)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    dataset = ArtDataset(output_dir, transform=transform)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

    # –®–∞–≥ 4: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    model = TinyUNet()
    diffusion = DiffusionProcess(T=100)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # –®–∞–≥ 5: –û–±—É—á–µ–Ω–∏–µ —Å Mixed Precision
    # train_model_with_mixed_precision(model, dataloader, diffusion, criterion, optimizer, device, target_loss=0.02, max_epochs=150)

    # –®–∞–≥ 6: –ó–∞–ø—É—Å–∫ GUI
    app = DiffusionGUI()
    app.mainloop()